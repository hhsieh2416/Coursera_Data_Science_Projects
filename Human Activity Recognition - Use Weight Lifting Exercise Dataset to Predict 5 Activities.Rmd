---
title: Human Activity Recognition - Use Weight Lifting Exercise Dataset to Predict
  5 Activities
author: "Han-Yu Hsieh"
date: "15/11/2020"
output: html_document
---

# Overview
In this project, I am interested in using data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to build a model to predict the possible 6 exercise activities( the "classe" variable in the training set). The data for this project come from this source: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har. More information is also available from the website. (see the section on the Weight Lifting Exercise Dataset). This is also the Coursera course project from Data Science: Statistics and Machine Learning Specialization by Johns Hopkins University.

## 1. Loading packages and reviewing the data
```{r results='hide', message = FALSE, warning = FALSE}
library(ggplot2);library(randomForest);library(caret);library(lattice)
train <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv', na.strings = c("","NA"))
test <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv', na.strings = c("","NA"))

str(train);str(test)
train$classe <- as.factor(train$classe)
```

```{r}
#remove NA and missing values in data set
p <- function(x) {sum(is.na(x))/length(x)*100}
apply(train, 2, p)
#it showed that there are variables with no missing values, and there are variables with 97.93% of missing values. So deleting the columns, who have more than 90% of missing values, is the next step
train <- train[, which(colMeans(!is.na(train))> 0.9)]
#as well as removed the 1st to 7th columns since the information is not relevant
train <- train[,-c(1:7)]
dim(train)
#do the same process with test data set
test <- test[, which(colMeans(!is.na(test)) > 0.9)]
dim(test)
```


## 2. Exploratory data analysis
```{r}
plot(train$classe)
```


## 3.1 Use Random Forests to build the prediction model
```{r}
set.seed(222)
rf <- randomForest(classe ~., data = train)
```

## 3.2 Review the model 
```{r}
#OOB error rate
print(rf)
```
From the model report, we can see the OOB estimate of error rate is 0.25% and mtry is 7. The OOB error rate is very good. No need to increase mtry

```{r}
#Error rate of Random Forest
plot(rf)
```

From the plot, when the ntree number is around 100, the error rate becomes steady. Actually, there is no need to increase ntree number.

```{r}
# Review important variables
varImpPlot(rf, sort = T, n.var = 20,
           main = "Top 20 - Variable Importance" )
```

## 3.3 Validate the random foresr model
```{r}
predict_RF <- predict(rf, train)
confusionMatrix(predict_RF, train$classe)
```
Overall, it is a very strong prediction model with only 0.25% OOB error rate, and 100% accuracy when I re-applied the model back to train data set. 


## 4.1 Use generalized boosted regression to build the prediction model
```{r}
set.seed(222)
customControl <- trainControl(method = 'repeatedcv', number = 5, repeats = 1)
modFit_GBR <- train(classe ~., train, method = 'gbm', trControl = customControl, verbose =FALSE)
print(modFit_GBR)
```
The final model has 96.33% accuracy with 5 fold cross-validation and 1 time repeat. 

## 4.2 Validate the generalized boosted regression model
```{r}
predict_GBR <- predict(modFit_GBR, train)
confusionMatrix(predict_GBR, train$classe)
```
Comparing these two models, the random forest model performs better with 100% accuracy when I re-applied train data set to the model, whereas the generalized boosted regression model has 97.26% accuracy. I am confident to use the random forest model to run a prediction of test data.


## 5. Prediction for the test data set
```{r}
predict_rf <- predict(rf, test)
predict_rf
```
